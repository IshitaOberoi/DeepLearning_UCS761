{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6n7OGAkGMPx",
        "outputId": "171dface-3468-444a-9883-899db9470e67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  experience  income\n",
            "0   25           1   30450\n",
            "1   30           3   35670\n",
            "2   47           2   31580\n",
            "3   32           5   40130\n",
            "4   43          10   47830\n",
            "Index(['age', 'experience', 'income'], dtype='object')\n",
            "(20, 3)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv(\"multiple_linear_regression_dataset.csv\")\n",
        "\n",
        "print(data.head())\n",
        "print(data.columns)\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Inputs: age and experience\n",
        "These features are used to predict salary.\n",
        "\n",
        "Output: income\n",
        "This is the value the model tries to predict.\n",
        "\n",
        "Number of features: 2\n",
        "So the model needs two weights.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "U9J5sT84KYQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inputs (features)\n",
        "X = data[[\"age\", \"experience\"]].values\n",
        "\n",
        "# Output (target)\n",
        "y = data[\"income\"].values"
      ],
      "metadata": {
        "id": "Psaj_PNAH1EL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Shape of X: (20, 2)\n",
        "There are 20 samples and 2 input features (age and experience).\n",
        "\n",
        "Shape of y: (20,)\n",
        "There are 20 target values, one salary for each sample.\n",
        "\n",
        "X has 2 columns because the model uses two input features.\n",
        "y has only one column because we predict only one output (income).\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ito8oGDlKgKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = X.shape[1]\n",
        "w = np.zeros(n_features)\n",
        "b = 0.0"
      ],
      "metadata": {
        "id": "NwUAJ_MMH1G1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We need one weight per feature because each input should have its own importance.\n",
        "Each weight tells how strongly that feature affects the salary prediction.\n",
        "\n",
        "Bias is separate because it shifts the prediction line up or down.\n",
        "It allows the model to make predictions even when all inputs are zero.\n",
        "\n",
        "Initializing with large values is risky because predictions may become very large,\n",
        "loss can explode, and training may become unstable or slow to converge.\n",
        "Small values make learning smoother and safer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Lf6IhwgTKnI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(X, w, b):\n",
        "    y_hat = X.dot(w) + b\n",
        "    return y_hat"
      ],
      "metadata": {
        "id": "t7evPAC6H1Jn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "There is no activation function because this is a regression problem.\n",
        "We want to predict a number (salary), not a class label.\n",
        "Activation functions like step or sigmoid restrict the output range,\n",
        "which is not suitable for numeric prediction.\n",
        "\n",
        "y_hat can take any real value.\n",
        "It can be small, large, positive, or decimal depending on the inputs.\n",
        "\n",
        "This is different from logistic regression because logistic regression\n",
        "uses a sigmoid function and outputs probabilities between 0 and 1.\n",
        "Here we directly output a number without any restriction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "3WEzeTqdLSGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_squared_error(y, y_hat):\n",
        "    loss = ((y_hat - y) ** 2).mean()\n",
        "    return loss"
      ],
      "metadata": {
        "id": "vrGFk3MwH1L8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We square the error to make all errors positive and to penalize large\n",
        "mistakes more strongly. Bigger errors get much larger penalties.\n",
        "\n",
        "If one prediction is very wrong, its squared error becomes very large,\n",
        "so the total loss increases a lot. This forces the model to correct it quickly.\n",
        "\n",
        "We do not just take absolute error because it is harder to optimize and\n",
        "not smooth for gradient descent. Squared error is smoother and easier for learning.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OUcJ1lEtLUon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(X, y, y_hat):\n",
        "    N = len(y)\n",
        "    dw = (2 / N) * X.T.dot(y_hat - y)\n",
        "    db = (2 / N) * (y_hat - y).sum()\n",
        "    return dw, db"
      ],
      "metadata": {
        "id": "aUT9PAQAH1OX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "X appears in dw because weights are multiplied with the input features.\n",
        "So the gradient of weights depends on both the error and the input values.\n",
        "\n",
        "X does not appear in db because bias is not multiplied by any feature.\n",
        "Bias is just a constant shift, so its gradient depends only on the error.\n",
        "\n",
        "The error term appears everywhere because learning is based on how wrong\n",
        "the prediction is. Bigger error means bigger correction in weights and bias.\n",
        "\n",
        "If error is zero, gradients become zero, weights do not change,\n",
        "and learning stops because the model is already correct.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "H0EGEvVcLVIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(w, b, dw, db, lr):\n",
        "    w = w - lr * dw\n",
        "    b = b - lr * db\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "G2SK0nD3H1Q4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0001\n",
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "    y_hat = predict(X, w, b)\n",
        "    loss = mean_squared_error(y, y_hat)\n",
        "    dw, db = compute_gradients(X, y, y_hat)\n",
        "    w, b = update_parameters(w, b, dw, db, lr)\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwDj_Iu1H1Ta",
        "outputId": "147bd0b6-f0a9-478a-83e0-95726af40848"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1727049635.0\n",
            "Epoch 100, Loss: 66491868.55311352\n",
            "Epoch 200, Loss: 61752567.201190114\n",
            "Epoch 300, Loss: 58616531.07847049\n",
            "Epoch 400, Loss: 56528801.53951118\n",
            "Epoch 500, Loss: 55126542.02946697\n",
            "Epoch 600, Loss: 54172526.94885703\n",
            "Epoch 700, Loss: 53511656.14292054\n",
            "Epoch 800, Loss: 53042523.72795741\n",
            "Epoch 900, Loss: 52698829.56325033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Yes, loss should decrease over time if learning is correct.\n",
        "This means the model is improving and predictions are getting closer to actual values.\n",
        "\n",
        "If loss increases, it usually means the learning rate is too high\n",
        "or the gradients are wrong. The updates may be too large and unstable.\n",
        "\n",
        "Learning rate and epochs work together:\n",
        "learning rate controls how big each step is,\n",
        "epochs control how many steps we take.\n",
        "Small learning rate needs more epochs,\n",
        "large learning rate needs fewer epochs but may be unstable.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_iojcgjZL2Kn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final weights:\", w)\n",
        "print(\"Final bias:\", b)\n",
        "new_candidate = np.array([4.5, 68])\n",
        "predicted_salary = new_candidate.dot(w) + b\n",
        "print(\"Predicted salary:\", predicted_salary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzA13vWbH1Wu",
        "outputId": "0184158b-82e7-486d-912d-f515c3f0364a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final weights: [ 764.75405919 1371.03430441]\n",
            "Final bias: 321.73641174472493\n",
            "Predicted salary: 96993.4623777421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Yes, the prediction is reasonable if it is close to the salary values\n",
        "seen in the dataset. It should not be extremely large or negative.\n",
        "\n",
        "Yes, it interpolates smoothly because linear regression produces a\n",
        "continuous straight-line relationship. Small changes in inputs lead\n",
        "to small changes in output.\n",
        "\n",
        "This is better than threshold rules because threshold rules give only\n",
        "fixed or step-like outputs, while regression gives precise numeric\n",
        "predictions and adapts smoothly to new inputs.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9LvAqMTGL3VD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}